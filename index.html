<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Finding NeMo : Negative-mined Mosaic Augmentation for Referring Image Segmentation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Finding NeMo : Negative-mined Mosaic Augmentation for Referring Image Segmentation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://seongsuha.github.io/" target="_blank">Seongsu Ha</a><sup>1,2*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Chaeyun Kim</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Donghwa Kim</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="FOURTH AUTHOR PERSONAL LINK" target="_blank">Junho Lee</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="FIFTH AUTHOR PERSONAL LINK" target="_blank">Sangho Lee</a><sup>3</sup>,</span>
                <span class="author-block">
                  <a href="http://www.joonseok.net/home.html" target="_blank">Joonseok Lee</a><sup>1,4</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup> 1</sup> Seoul National University </span>
                    <span class="author-block">
                      <sup> 2</sup> Twelve Labs </span>
                    <span class="author-block">
                      <sup> 3</sup> Allen Institute for AI </span>
                    <span class="author-block">
                      <sup> 4</sup> Google Research </span>
                      <br>ECCV 2024</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
<!--                     <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/snuviplab/NeMo" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body" style="text-align: center;"> <!-- Add text-align center -->
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video> -->
      <img src="static/images/overview.jpeg" width="100%" class="center" style="display: block; margin: 0 auto;"> <!-- Center the image -->
      <h2 class="subtitle has-text-centered">
        <b>Negative-mined Mosaic Augmentation (NeMo)</b> generates ambiguous examples where a model is encouraged to concretely understand the scene and the query. In the original image, understanding “a woman” is enough to find the target, but in the augmented image, the model needs to understand what “woman in front of the wall” means.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Referring Image Segmentation is a comprehensive task to segment an object referred by a textual query from an image. In nature, the level of difficulty in this task is affected by the existence of similar objects and the complexity of the referring expression. Recent RIS models still show a significant performance gap between easy and hard scenarios. We pose that the bottleneck exists in the data, and propose a simple but powerful data augmentation method, Negative-mined Mosaic Augmentation (NeMo). This method augments a training image into a mosaic with three other negative images carefully curated by a pretrained multimodal alignment model, e.g., CLIP, to make the sample more challenging. We discover that it is critical to properly adjust the difficulty level, neither too ambiguous nor too trivial. The augmented training data encourages the RIS model to recognize subtle differences and relationships between similar visual entities and to concretely understand the whole expression to locate the right target better. Our approach shows consistent improvements on various datasets and models, verified by extensive experiments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero is-small">
  <div class="hero-body"> 
    <div class="container is-max-desktop">
      <div class="column is-centered">
        <div class="column is-full">
          <h2 class="title is-3">Referring Image Segmentation</h2>
          <div class="content has-text-justified">
            <p>
    <!--           Referring Image Segmentation segments an object when you are given an image and a free-form query referring to that certain target object. What differentiates RIS from other grounding tasks is that the scene is usually given in the way that visually similar objects coexist. Due to this nature of RIS task, we find difficulty levels in RIS can change depending on the number of negative objects that look similar to the target referent. The more negative objects you have, the more difficult the RIS problem becomes and the model needs to fully understand the words to discern similar objects. -->
              Referring Image Segmentation predicts a segmentation mask of the object referred with a given an image and a text. The key to RIS task is to discern the referent among visually similar objects via textual cues. The difficulty of each RIS scenario can be affected by the degree of visual ambiguity in the scene given the linguistic complexity of the referring expression. For example, in (a), query (1) demands discernment among three road signs, while query (2) involves identifying a “woman”, relatively easier due to a single instance.
            </p>
        <img src="static/images/intro1.jpeg" width="50%" class="center" style="display: block; margin: 0 auto;">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small is-light">
  <div class="hero-body"> 
    <div class="container is-max-desktop">
      <div class="column is-centered"> <!--has-text-centered-->
        <div class="column is-full">
          <h2 class="title is-3">Do current RIS models actually perform well in hard scenarios?</h2>
          <div class="content has-text-justified">
            <p>
          We manually pick 100 easy and hard samples depending on the number of negative objects. A huge performance gap exists between easy & hard examples in current models. Variant Inter and even Intra-dataset grounding difficulty levels exist in training data as well. We ask if these samples are challenging enough to discern subtle visual and textual nuance for RIS.
<!--           We evaluate if the current models actually perform well in hard cases by manually picking examples by considering the number of negative objects. Surprisingly, we find there is a huge performance gap between easy and hard scenarios. To find a possible cause, we have inspected training samples, and found many easy samples even within the dataset that are thought to be harder, once we also consider the degree of visual ambiguity in the scene given the query. We assume these samples may not be challenging enough to train models for capabilities that RIS needs. -->
        </p>
        <img src="static/images/easy_hard.png" width="80%" class="center-image">
        <img src="static/images/performance_gap.png" width="50%" class="center-image">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body"> 
    <div class="container is-max-desktop">
      <div class="column is-centered">
        <div class="column is-full">
          <h2 class="title is-3">Overall Pipeline</h2>
          <div class="content has-text-justified">
            <p>Given an image and a query, it selects negative images at a proper level of difficulty, filtering out visually or semantically images to the query to avoid false negatives and irrelevant (easy) images identified by text-to- image retrieval. It randomly selects three among the remaining to construct a mosaic.</p>
        <img src="static/images/pipeline.jpeg" width="80%" class="center" style="display: block; margin: 0 auto;">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small is-light">
  <div class="hero-body"> 
    <div class="container is-max-desktop">
      <div class="column is-centered">
        <div class="column is-full">
          <h2 class="title is-3">Overall Comparison</h2>
          <div class="content has-text-justified">
            <p>
         We observe a larger performance boost on more complex datasets.  Harder datasets benefit more because of its intricate referring expressions and visually dense scenes.
        </p>
        <img src="static/images/overall_comparison.png" width="100%" class="center" style="display: block; margin: 0 auto;">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body"> 
    <div class="container is-max-desktop">
      <div class="column is-centered">
        <div class="column is-full">
          <h2 class="title is-3">Comparison to other augmentation methods</h2>
          <div class="content has-text-justified">
            <p> Our approach performed best, as it preserves the original context needed for referring image segmentation, while the other methods often disrupt key visual elements.
        </p>
        <img src="static/images/other_aug.png" width="70%" class="center" style="display: block; margin: 0 auto;">
        <img src="static/images/other_aug_table.png" width="50%" class="center" style="display: block; margin: 0 auto;">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small is-light">
  <div class="hero-body"> 
    <div class="container is-max-desktop">
      <div class="column is-centered">
        <div class="column is-full">
          <h2 class="title is-3">Detailed Analysis of the Proposed Method</h2>
          <div class="content has-text-justified">
            <p> 
        </p>
<!--         <img src="static/images/overall_comparison.jpeg" width="100%" class="center" style="display: block; margin: 0 auto;"> -->
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body"> 
    <div class="container is-max-desktop">
      <div class="column is-centered">
        <div class="column is-full">
          <h2 class="title is-3">Qualitative Results</h2>
          <div class="content has-text-justified">
            <p> 
              Qualitative results demonstrates that (a) our method successfully detects a blurred and small “person” in the background, positioned behind the most prominent person in the image. (b) Our method also segments the entire dish while the baseline only detects the left half without fully understanding the query describing the right half. Also, (c) it is apparent that our method yields an output with more distinct shape for “the second” horse. (d) Our method captures objects more accurately in scenarios involving directional expressions, indicating improved understand- ing both in absolute and relative positions. 
        </p>
        <img src="static/images/qual_res.png" width="100%" class="center" style="display: block; margin: 0 auto;">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">

        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">

        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">

        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">

      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->




<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/watch?v=Z0nT51OSLNA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">

            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">

            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\

            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
